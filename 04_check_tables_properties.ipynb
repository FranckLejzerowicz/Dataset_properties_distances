{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from functools import reduce\n",
    "from biom import load_table\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy\n",
    "from fitter import Fitter\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# import random\n",
    "\n",
    "# from memory_profiler import profile\n",
    "# from memory_profiler import memory_usage\n",
    "\n",
    "# %reload_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from functs import remove_per_paper,get_marginals,get_sparsity,get_span_countProp,nskip_header,get_f_dist,get_feats_fits,get_distrib_freqs_per_feature,get_distrib_freqs_vec,get_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the parada17 paper - always remove the samples which have \"DOBD\" in mock_or_monthlies\"\n",
    "def remove_per_paper(table_pd, meta_pd):\n",
    "    \"\"\"\n",
    "    For the passed table, treat specifically depending on the qiita_paper:\n",
    "        - if there is a \"misc environment\" in the metadata for this dataset:\n",
    "            -> remove the BLANKS / CONTROLS / MOCKS by keeping assays\n",
    "    Returns the filtered table\n",
    "    \"\"\"\n",
    "    if 'misc environment' in meta_pd.env_package.unique():\n",
    "        samples_to_keep = set(meta_pd.loc[meta_pd['env_package'] == 'misc environment',:].index.tolist())\n",
    "        table_pd = table_pd.loc[:, [x for x in table_pd.columns if '.'.join(x.split('.')[1:]) not in samples_to_keep]]\n",
    "        table_pd = table_pd[(table_pd.T != 0).any()]\n",
    "    else:\n",
    "        print('no blank/mock')\n",
    "    return table_pd\n",
    "\n",
    "def get_marginals(tab):\n",
    "    \"\"\"\n",
    "    For the passed table, get:\n",
    "        - the sums of reads per sample (must be columns)\n",
    "        - the sums of reads per features (must be rows)\n",
    "    Returns (min, max) for both\n",
    "    \"\"\"\n",
    "    samplesSums = round(tab.sum(0), 3)\n",
    "    featuresSums = round(tab.sum(1), 3)\n",
    "    return (min(featuresSums), max(featuresSums)), (min(samplesSums), max(samplesSums))\n",
    "\n",
    "def get_sparsity(tab):\n",
    "    \"\"\"\n",
    "    For the passed table, get:\n",
    "        - total number of entries\n",
    "        - total number of zeros\n",
    "        - total number of ones\n",
    "        - sparsity (as percent zeros)\n",
    "    Returns these four properties\n",
    "    \"\"\"\n",
    "    NN = tab.shape[0] * tab.shape[1]\n",
    "    ZR = (tab==0).sum().sum()\n",
    "    ONE = (tab==1).sum().sum()\n",
    "    sparsity = round((ZR/NN)*100, 3)\n",
    "    return NN, ZR, ONE, sparsity\n",
    "\n",
    "def get_span_countProp(tab, min_max_samples_sums):\n",
    "    \"\"\"\n",
    "    Checks if binary and if not get the actual values\n",
    "      binary_count_or_prop could be:\n",
    "       - [0,1] for binary\n",
    "       - [n1,n2,...n10] for count (.hist())\n",
    "       - [0.,1.] or [0.,100.] for prop\n",
    "    Returns count and type\n",
    "    \"\"\"\n",
    "    # set data_type as 'float' or 'int' if all the columns are integers\n",
    "    data_type = 'float'\n",
    "    int_cols = tab.apply(lambda x: pd.to_numeric(x, errors='coerce').notnull().all())\n",
    "    if int_cols.sum() == tab.shape[1]:\n",
    "        data_type = 'int'\n",
    "\n",
    "    # if the data_type is still 'float' \n",
    "    if data_type == 'float':\n",
    "        if min(min_max_samples_sums) >= 0:\n",
    "            if min(min_max_samples_sums) <= 1.:\n",
    "                return [0., 1.], 'prop'\n",
    "            elif min(min_max_samples_sums) <= 100.:\n",
    "                return [0., 100.], 'prop'\n",
    "            else:\n",
    "                return [0., min_max_samples_sums[1]], 'prop'\n",
    "        \n",
    "    # get the frequencies if the table in 10 bins\n",
    "    count, division = np.histogram(tab)\n",
    "    return list(count), 'count'\n",
    "\n",
    "def nskip_header(tab):\n",
    "    \"\"\"\n",
    "    Checks whether the passed table starts\n",
    "        by the biom convert --to-tsv header\n",
    "    Returns 0 if no, 1 if yes\n",
    "    \"\"\"\n",
    "    with open(tab) as f:\n",
    "        for line in f:\n",
    "            if line.startswith('# Constructed from biom file'):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "\n",
    "def get_f_dist(cur_pd, feat, dist_names):\n",
    "    \"\"\"\n",
    "    For the passed feature,\n",
    "     get the sumsquare_error of the\n",
    "     fit on each distribution as dict:\n",
    "        - key   = distribution name\n",
    "        - value = sumsqure_error\n",
    "    Returns this dict\n",
    "    \"\"\"\n",
    "    f = Fitter(cur_pd.loc[feat,], distributions=dist_names, verbose=False)\n",
    "    f.fit()\n",
    "    f_dist = f.summary(Nbest=len(dist_names)).to_dict()['sumsquare_error']\n",
    "    return f_dist\n",
    "\n",
    "def get_feats_fits(cur_pd, dist_names):\n",
    "    \"\"\"\n",
    "    For the passed table, get a dict:\n",
    "        - key   = feature names\n",
    "        - value = sumsquare errors for each distribution\n",
    "    Returns the dict\n",
    "    \"\"\"\n",
    "    # get the features names\n",
    "    cur_pd_list = cur_pd.index.tolist()\n",
    "\n",
    "    ## get 100 randomly picked features\n",
    "    # cur_pd_list = random.sample(cur_pd_list,10)\n",
    "    \n",
    "    # for each feature, fill list with the SSE on each distribution\n",
    "    feats_fits = dict([x, []] for x in dist_names)\n",
    "    for fdx, feat in enumerate(cur_pd_list):\n",
    "\n",
    "\n",
    "        # if fdx == 10:\n",
    "        #     break\n",
    "\n",
    "\n",
    "        # get the dict {'distrib': 'SSE'}  for the current feature\n",
    "        f_dist = get_f_dist(cur_pd, feat, dist_names)\n",
    "        # for each distribution, \n",
    "        for dist in dist_names:\n",
    "            feats_fits[dist].append(f_dist[dist])\n",
    "    return feats_fits\n",
    "\n",
    "def get_distrib_freqs_per_feature(feats_fits):\n",
    "    \"\"\"\n",
    "    For the passed feature -> SSEs dict get:\n",
    "        - the 2D array or 0 to 1 rows for each feature\n",
    "    Returns this array as a pandas dataframe\n",
    "    \"\"\"\n",
    "    # init list and read dict as table\n",
    "    df_frqs = []\n",
    "    df = pd.DataFrame(feats_fits)\n",
    "    # for each feature vectoir of distribution errors\n",
    "    for L in df.values:\n",
    "        # get the mini and maxi error\n",
    "        mini, maxi = min(L), max(L)\n",
    "        # bin these errors along a 10-bins logspace\n",
    "        lin = np.logspace(np.log10(mini), np.log10(maxi), num=10)\n",
    "        dig = np.digitize(L,lin)\n",
    "        # get the minimum bin value (best / first rank)\n",
    "        min_rank = min(dig)\n",
    "        # make ties for this rank\n",
    "        n1 = list(dig).count(min_rank)\n",
    "        if n1 > 1:\n",
    "            v = 1/n1\n",
    "            df_frq = [v if x == min_rank else 0 for x in dig]\n",
    "        else:\n",
    "            df_frq = [1 if x == min_rank else 0 for x in dig]\n",
    "        # collect the frequencies that sum to one for the best distribution(s)\n",
    "        df_frqs.append(df_frq)\n",
    "    df_frqs_pd = pd.DataFrame(df_frqs)\n",
    "    return df_frqs_pd\n",
    "\n",
    "def get_distrib_freqs_vec(df_frqs_pd):\n",
    "    \"\"\"\n",
    "    For the passed distribution ranking table,\n",
    "     get the probability for each distribution across features\n",
    "    Returns the 1D vector of distribution probabilities\n",
    "    \"\"\"\n",
    "    # make the sum of the first ranks per columm\n",
    "    # (i.e. frequencies of distribution across features)\n",
    "    df_frqs_sum = df_frqs_pd.sum(0)\n",
    "    # make it a probability\n",
    "    df_frqs_sum_p = df_frqs_sum/df_frqs_sum.sum()\n",
    "    return df_frqs_sum_p.tolist()\n",
    "\n",
    "def get_distrib_freqs_data(tab, dist_names):\n",
    "    # get the dict with for each feature the vector SSE to above distributions\n",
    "    feats_fits = get_feats_fits(tab, dist_names)\n",
    "    # get the 2D array with 0 to 1 rank for the best distribution(s) per feature\n",
    "    distrib_freqs = get_distrib_freqs_per_feature(feats_fits)\n",
    "    # get the 1D vector of probabilities for each distribution\n",
    "    distrib_freqs_vec = get_distrib_freqs_vec(distrib_freqs)\n",
    "    return distrib_freqs_vec\n",
    "\n",
    "def get_meta_pd(table):\n",
    "    # collect metadata for the qiita papers tables (not yet used)\n",
    "    if '/qiita_papers/' in table:\n",
    "        paper = table.split('/')[-4]\n",
    "        artifact = table.split('/')[-2]\n",
    "        meta_fp = '%s/qiita/%s/qiita_processed/mapping_files/%s_mapping_file.txt' % (table.split('/datasets/qiita_papers/')[0], paper, artifact)\n",
    "        meta_pd = pd.read_csv(meta_fp, header=0, index_col=0, sep='\\t')\n",
    "        return meta_pd\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def collect_1_2_3(props_per_dataset, min_max_features_sums, min_max_samples_sums,\n",
    "                  n_entries, one_entries, zero_entries, zero_entries_perc,\n",
    "                  number_span, binary_count_or_prop):\n",
    "    \"\"\"\n",
    "    Just populate a dict\n",
    "    \"\"\"\n",
    "    props_per_dataset.setdefault('min_features_sums', []).append(min_max_features_sums[0])\n",
    "    props_per_dataset.setdefault('max_features_sums', []).append(min_max_features_sums[1])\n",
    "    props_per_dataset.setdefault('min_samples_sums', []).append(min_max_samples_sums[0])\n",
    "    props_per_dataset.setdefault('max_samples_sums', []).append(min_max_samples_sums[1])\n",
    "    props_per_dataset.setdefault('n_entries', []).append(n_entries)\n",
    "    props_per_dataset.setdefault('one_entries', []).append(one_entries)\n",
    "    props_per_dataset.setdefault('zero_entries', []).append(zero_entries)\n",
    "    props_per_dataset.setdefault('zero_entries_perc', []).append(zero_entries_perc)\n",
    "    props_per_dataset.setdefault('number_span', []).append('-'.join(map(str, number_span)))\n",
    "    props_per_dataset.setdefault('binary_count_or_prop', []).append(binary_count_or_prop)\n",
    "    return props_per_dataset\n",
    "\n",
    "def write_basic(props_per_dataset, out_prop_fp, tables_chunk):\n",
    "    \"\"\"\n",
    "    Writter\n",
    "    \"\"\"\n",
    "    # make a main table and add the file column\n",
    "    props_per_dataset_pd = pd.DataFrame(props_per_dataset)\n",
    "    props_per_dataset_pd['file'] = tables_chunk\n",
    "    props_per_dataset_pd.to_csv(out_prop_fp, index=False, sep='\\t')\n",
    "    print('Written:', out_prop_fp)\n",
    "\n",
    "def write_distrbutions(distrib_freqs_per_dataset, dist_names,\n",
    "                       tables_chunk, addon, out_prop_JS_fp):\n",
    "    \"\"\"\n",
    "    Another writter that may append\n",
    "    \"\"\"\n",
    "    if len(distrib_freqs_per_dataset):\n",
    "        # make a pandas dataframe from all the probability vectors snd add the file column\n",
    "        distrib_freqs_per_dataset_pd = pd.DataFrame(distrib_freqs_per_dataset, columns=dist_names)\n",
    "        distrib_freqs_per_dataset_pd['file'] = tables_chunk\n",
    "        # write only the lines for the files not already measured\n",
    "        if addon:\n",
    "            # read the already written file and concatenate the new neasures to it\n",
    "            distrib_freqs_per_dataset_pd_0 = pd.read_csv(out_prop_JS_fp, header=0, sep='\\t')\n",
    "            distrib_freqs_per_dataset_pd_1 = pd.concat([distrib_freqs_per_dataset_pd_0, distrib_freqs_per_dataset_pd])\n",
    "            distrib_freqs_per_dataset_pd_1.to_csv(out_prop_JS_fp, index=False, sep='\\t')\n",
    "        # write entire output if all distribution probability measured de novo\n",
    "        else:\n",
    "            distrib_freqs_per_dataset_pd.to_csv(out_prop_JS_fp, index=False, sep='\\t')\n",
    "    print('Written:', out_prop_JS_fp)\n",
    "\n",
    "def run_prop_catcher(tdx, tables_chunk, tables_chunk_toJS,\n",
    "                     out_prop_fp, out_prop_JS_fp, addon):\n",
    "    \"\"\"\n",
    "    This is the nain worker that call functions for each property to collect:\n",
    "        (1) marginal sums ((min, max) for both samples and features)\n",
    "        (2) sparsity (number of entries, number of zeros, number of ones, percent zeros)\n",
    "        (3) read counts span and data_type\n",
    "        (4) feature distribtion sumsquare_errors\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare a dict that will contain other properties\n",
    "    props_per_dataset = {}\n",
    "    # prepare a list that will contain the lists of distributions probabilities for J-S\n",
    "    distrib_freqs_per_dataset = []\n",
    "\n",
    "    # for each tsv'ed dataset file\n",
    "    for table in tables_chunk:\n",
    "        meta_pd = get_meta_pd(table)\n",
    "        # read the dataset (may skip first row if converted from BIOM)\n",
    "        tab = pd.read_csv(table, skiprows=nskip_header(table), header=0, index_col=0, sep='\\t')\n",
    "        if 'qiita_paper' in table:\n",
    "            tab = remove_per_paper(table_pd, meta_pd)\n",
    "        \n",
    "        # (1) get marginal sums\n",
    "        min_max_features_sums, min_max_samples_sums = get_marginals(tab)\n",
    "        # (2) get sparsity \n",
    "        n_entries, zero_entries, one_entries, zero_entries_perc = get_sparsity(tab)\n",
    "        # (3) get span of read counts and string on the type of data\n",
    "        number_span, binary_count_or_prop = [0,1], 'binary'\n",
    "        if (zero_entries + one_entries) != n_entries:\n",
    "            number_span, binary_count_or_prop = get_span_countProp(tab, min_max_samples_sums)\n",
    "        \n",
    "        # collect the (1) (2) and (3) properties in a dict\n",
    "        props_per_dataset = collect_1_2_3(props_per_dataset, min_max_features_sums,\n",
    "                                          min_max_samples_sums, n_entries, one_entries,\n",
    "                                          zero_entries, zero_entries_perc,\n",
    "                                          number_span, binary_count_or_prop)\n",
    "        \n",
    "        # (4) feature distribtion sumsquare_errors\n",
    "        # distributions to look for features probabilities \n",
    "\n",
    "\n",
    "        # dist_names = ['cauchy', 'chi2', 'cosine'] \n",
    "\n",
    "\n",
    "        dist_names = ['cauchy', 'chi2', 'cosine', 'expon',\n",
    "                      'gamma', 'gausshyper', 'genhalflogistic',\n",
    "                      'gompertz', 'halfcauchy', 'halflogistic',\n",
    "                      'halfnorm', 'logistic', 'lognorm',\n",
    "                      'nakagami', 'norm', 'pareto', 'powerlaw',\n",
    "                      'semicircular', 't', 'triang',\n",
    "                      'tukeylambda', 'uniform']\n",
    "        if os.path.isfile(out_prop_JS_fp):\n",
    "            with open(out_prop_JS_fp) as f:\n",
    "                for line in f:\n",
    "                    break\n",
    "            if dist_names != line.split()[:-1]:\n",
    "                print('Cannot concatenate different distance lists: rewritting')\n",
    "                addon = 0\n",
    "                distrib_freqs_vec = get_distrib_freqs_data(tab, dist_names)\n",
    "            elif tab not in tables_chunk_toJS:\n",
    "                distrib_freqs_vec = get_distrib_freqs_data(tab, dist_names)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            distrib_freqs_vec = get_distrib_freqs_data(tab, dist_names)\n",
    "        distrib_freqs_per_dataset.append(distrib_freqs_vec)\n",
    "\n",
    "    ## write basic properties:\n",
    "    write_basic(props_per_dataset, out_prop_fp, tables_chunk)\n",
    "    ## features distribution properties:\n",
    "    write_distrbutions(distrib_freqs_per_dataset, dist_names,\n",
    "                       tables_chunk, addon, out_prop_JS_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append the tsv'ed biome tables of each dataset into a list\n",
    "tables_list = []\n",
    "ntabs = 0\n",
    "for root, dirs, files in os.walk('/home/flejzerowicz/netbench/datasets'):\n",
    "    for fil in files:\n",
    "        # skip non-tsv'ed biom tables\n",
    "        if 'explanation' in fil:\n",
    "            continue\n",
    "        # only look into the actual folders containing tables\n",
    "        if 'qiita_papers' not in root and 'weiss_paper' not in root:\n",
    "            continue\n",
    "        # make shure it's a tsv (here txt, dang!)\n",
    "        if fil.split('.')[-1] == 'txt':\n",
    "            tab = root + '/' + fil\n",
    "            paper_or_weiss = '/'.join(root.split('/datasets/')[-1].split('/')[:3])\n",
    "            ntabs += 1\n",
    "            tables_list.append(tab)\n",
    "\n",
    "# splits this list into a list of 12 lists corresponding to chunks of files\n",
    "tables_chunks = [list(x) for x in np.array_split(np.array(sorted(tables_list)[::-1]), 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdx ['/home/flejzerowicz/netbench/datasets/weiss_paper/ts_4/txts/table_9.biom.txt', '/home/flejzerowicz/netbench/datasets/weiss_paper/ts_4/txts/table_8.biom.txt', '/home/flejzerowicz/netbench/datasets/weiss_paper/ts_4/txts/table_7.biom.txt']\n",
      "tdx ['/home/flejzerowicz/netbench/datasets/weiss_paper/ts_4/txts/table_1.biom.txt', '/home/flejzerowicz/netbench/datasets/weiss_paper/ts_3/txts/table_9.biom.txt', '/home/flejzerowicz/netbench/datasets/weiss_paper/ts_3/txts/table_8.biom.txt']\n",
      "Written: /home/flejzerowicz/netbench/datasets/properties/ck_0/tab_props.tsv\n",
      "Written: /home/flejzerowicz/netbench/datasets/properties/ck_0/JS.tsv\n",
      "Written: /home/flejzerowicz/netbench/datasets/properties/ck_1/tab_props.tsv\n",
      "Written: /home/flejzerowicz/netbench/datasets/properties/ck_1/JS.tsv\n"
     ]
    }
   ],
   "source": [
    "def run(tables_chunks):\n",
    "    \"\"\"\n",
    "    This function spawns a process for each files chunk (\"ck_#\") using multiprocessing\n",
    "    Each process runs the worker function \"run_prop_catcher\":\n",
    "        - it write files which names are created here in order to be collected and merged later:\n",
    "            * trivial dataset properties:\n",
    "                /home/flejzerowicz/netbench/datasets/properties/ck_#/tab_props.tsv\n",
    "            * features distribution for Jensen-Shannon:\n",
    "                /home/flejzerowicz/netbench/datasets/properties/ck_#/JS.tsv\n",
    "    Returns the list of files containing the per-chunk distrtibutions for Jensen-Shannon\n",
    "    \"\"\"\n",
    "    \n",
    "    # collect the output files to merge later\n",
    "    out_prop_fps = []\n",
    "    out_prop_JS_fps = []\n",
    "    # for mulitprocessing\n",
    "    jobs = []\n",
    "    # for each indexed datasets files chunk\n",
    "    for tdx, tables_chunk in enumerate(tables_chunks):\n",
    "\n",
    "\n",
    "        # tables_chunk = tables_chunk[:3]\n",
    "\n",
    "\n",
    "        # declare the output file names\n",
    "        out_prop_fp = '/home/flejzerowicz/netbench/datasets/properties/ck_%s/tab_props.tsv' % tdx\n",
    "        out_prop_JS_fp = '/home/flejzerowicz/netbench/datasets/properties/ck_%s/JS.tsv' % tdx\n",
    "        # create folder if need be\n",
    "        out_prop_dir = os.path.dirname(out_prop_JS_fp)\n",
    "        if not os.path.isdir(out_prop_dir):\n",
    "            os.makedirs(out_prop_dir)\n",
    "        # simply write the names of processed files of the chunk into the chunk folder for tracability\n",
    "        with open('/home/flejzerowicz/netbench/datasets/properties/ck_%s/files.tsv' % tdx, 'w') as o:\n",
    "            for table in tables_chunk:\n",
    "                o.write('%s\\n' % table)\n",
    "                \n",
    "        # Jensen-Shannon measure (heavy compute)\n",
    "        #  - hence collect a boolean if some datasets already measured\n",
    "        addon = 0\n",
    "        tables_chunk_toJS = tables_chunk\n",
    "        if os.path.isfile(out_prop_JS_fp):\n",
    "            # read it and compare the measured datasets to the datasets in the chunk\n",
    "            out_prop_JS_pd = pd.read_csv(out_prop_JS_fp, header=0, sep='\\t')\n",
    "            # get the files that have not been measured\n",
    "            tables_chunk_JSed = out_prop_JS_pd['file'].tolist()\n",
    "            tables_chunk_toJS = [x for x in tables_chunk if x not in tables_chunk_JSed]\n",
    "\n",
    "        # spawn the process\n",
    "        p = mp.Process(target=run_prop_catcher, args=(tdx, tables_chunk, tables_chunk_toJS,\n",
    "                                                      out_prop_fp, out_prop_JS_fp, addon))\n",
    "        # collect the JS output file\n",
    "        out_prop_fps.append(out_prop_fp)\n",
    "        out_prop_JS_fps.append(out_prop_JS_fp)\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "\n",
    "        # if tdx == 1:\n",
    "        #     break\n",
    "\n",
    "\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "\n",
    "    return out_prop_fps, out_prop_JS_fps\n",
    "\n",
    "out_prop_fps, out_prop_JS_fps = run(tables_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: /home/flejzerowicz/netbench/datasets/properties/props.tsv\n",
      "Written: /home/flejzerowicz/netbench/datasets/properties/JS.tsv\n",
      "Written: /home/flejzerowicz/netbench/datasets/properties/JS_mat.tsv\n"
     ]
    }
   ],
   "source": [
    "def get_JS(out_prop_JS_pd):\n",
    "    \"\"\"\n",
    "    Compute the Jensen-Shannon distances\n",
    "    Returns them in column-formatted distances, columns:\n",
    "        - file1\n",
    "        - file2\n",
    "        - JS_sym\n",
    "    \"\"\"\n",
    "    # init list to be the matrix\n",
    "    all_datasets_pairs = []\n",
    "    # for each pair of datasets\n",
    "    for it in itertools.combinations_with_replacement(out_prop_JS_pd.index.tolist(),2):\n",
    "        # sorted B-A to A-B\n",
    "        i = sorted(it)\n",
    "        # add a pseudocount to the probabilities\n",
    "        d1 = out_prop_JS_pd.loc[i[0],].values + 0.00000000001\n",
    "        d2 = out_prop_JS_pd.loc[i[1],].values + 0.00000000001\n",
    "        # get the actual distance and collect it\n",
    "        JS_sym = distance.jensenshannon(d1,d2)\n",
    "        # JS_sym = ( (scipy.stats.entropy(d1,d2) + scipy.stats.entropy(d2,d1)) / 2 )\n",
    "        row = i + [JS_sym]\n",
    "        all_datasets_pairs.append(row)\n",
    "    # make and write the column-formatted distances\n",
    "    all_datasets_pairs_pd = pd.DataFrame(all_datasets_pairs, columns = ['file1', 'file2', 'JS_sym'])\n",
    "    all_datasets_pairs_pd['file1'] = ['_'.join(os.path.splitext(x)[0].split('netbench/datasets/')[-1].split('/')[1:]) for x in all_datasets_pairs_pd['file1'].tolist()]\n",
    "    all_datasets_pairs_pd['file2'] = ['_'.join(os.path.splitext(x)[0].split('netbench/datasets/')[-1].split('/')[1:]) for x in all_datasets_pairs_pd['file2'].tolist()]\n",
    "    all_datasets_pairs_pd.to_csv('/home/flejzerowicz/netbench/datasets/properties/JS_table.tsv', index=False, sep='\\t')\n",
    "    return all_datasets_pairs_pd\n",
    "\n",
    "def write_mains(out_prop_fps, out_prop_JS_fps):\n",
    "    \"\"\"\n",
    "    Concatenate the probability tables and compute the Jensen-Shannon distances between datasets\n",
    "    \"\"\"\n",
    "    # get the JS divergences in long, column format\n",
    "    main_fp = '/home/flejzerowicz/netbench/datasets/properties/props.tsv'\n",
    "    out_prop_pd = pd.concat([pd.read_csv(out_prop_fp, header=0, sep='\\t') for out_prop_fp in out_prop_fps])\n",
    "    out_prop_pd.set_index('file', inplace=True)\n",
    "    out_prop_pd.fillna(0.0).to_csv(main_fp, index=True, sep='\\t')\n",
    "    print('Written:', main_fp)\n",
    "\n",
    "    # get the JS divergences in long, column format\n",
    "    main_JS_fp = '/home/flejzerowicz/netbench/datasets/properties/JS.tsv'\n",
    "    out_prop_JS_pd = pd.concat([pd.read_csv(out_prop_JS_fp, header=0, sep='\\t') for out_prop_JS_fp in out_prop_JS_fps])\n",
    "    out_prop_JS_pd.set_index('file', inplace=True)\n",
    "    out_prop_JS_pd.fillna(0.0).to_csv(main_JS_fp, index=True, sep='\\t')\n",
    "    print('Written:', main_JS_fp)\n",
    "\n",
    "    # make symetric matrix\n",
    "    JS_mat = get_JS(out_prop_JS_pd)\n",
    "    JS_mat = JS_mat.set_index(['file1', 'file2']).unstack()\n",
    "    JS_mat.columns = JS_mat.columns.droplevel()\n",
    "    for i in range(JS_mat.shape[0]):\n",
    "        for j in range(i, JS_mat.shape[0]):\n",
    "            JS_mat.iloc[j,i] = JS_mat.iloc[i,j]\n",
    "    del JS_mat.index.name\n",
    "    JS_mat_fp = '/home/flejzerowicz/netbench/datasets/properties/JS_mat.tsv'\n",
    "    print('Written:', JS_mat_fp)\n",
    "    JS_mat.to_csv(JS_mat_fp, index=True, sep='\\t')\n",
    "\n",
    "write_mains(out_prop_fps, out_prop_JS_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Metadata for qiime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_props_fp = '/home/flejzerowicz/netbench/datasets/properties/props.tsv'\n",
    "datasets_props_pd = pd.read_csv(datasets_props_fp, header=0, sep='\\t')\n",
    "\n",
    "datasets_meta_L = []\n",
    "for table in tables_list:\n",
    "    table_path_split = os.path.splitext(table)[0].split('netbench/datasets/')[-1].split('/')\n",
    "    main_type = table_path_split[0]\n",
    "    if main_type == 'qiita_papers':\n",
    "        raw_filt = table_path_split[1]\n",
    "        if raw_filt == 'filt':\n",
    "            filt = table_path_split[2]\n",
    "            dat = table_path_split[3]\n",
    "        else:\n",
    "            filt = 'NaN'\n",
    "            dat = table_path_split[2]\n",
    "    else:\n",
    "        raw_filt = 'NaN'\n",
    "        filt = 'NaN'\n",
    "        dat = table_path_split[1]\n",
    "    sam = '_'.join(table_path_split[1:])\n",
    "    datasets_meta_L.append([sam, main_type, raw_filt, filt, dat, table])\n",
    "datasets_meta_pd = pd.DataFrame(datasets_meta_L, columns=['sample_name', 'main_type', 'raw_filt', 'filtering', 'dataset', 'file'])\n",
    "datasets_meta_pd = datasets_meta_pd.merge(datasets_props_pd)\n",
    "datasets_meta_fp = '/home/flejzerowicz/netbench/datasets/properties/JS_mat_meta.tsv'\n",
    "datasets_meta_pd.to_csv(datasets_meta_fp, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mImported /home/flejzerowicz/netbench/datasets/properties/JS_mat.tsv as DistanceMatrixDirectoryFormat to /home/flejzerowicz/netbench/datasets/properties/JS_mat.qza\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!qiime tools import --input-path /home/flejzerowicz/netbench/datasets/properties/JS_mat.tsv --output-path /home/flejzerowicz/netbench/datasets/properties/JS_mat.qza --type DistanceMatrix\n",
    "!qiime diversity pcoa --i-distance-matrix /home/flejzerowicz/netbench/datasets/properties/JS_mat.qza --o-pcoa /home/flejzerowicz/netbench/datasets/properties/JS_mat_PCoA.qza\n",
    "!qiime emperor plot --i-pcoa /home/flejzerowicz/netbench/datasets/properties/JS_mat_PCoA.qza \\\n",
    "                    --m-metadata-file /home/flejzerowicz/netbench/datasets/properties/JS_mat_meta.tsv \\\n",
    "                    --o-visualization /home/flejzerowicz/netbench/datasets/properties/JS_mat_PCoA_emp.qzv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
